# -*- coding: utf-8 -*-
"""Relocating_NER_copyJavi_CLEAN-version.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wEFgzA0JVe6yiZJ45KVNeWeU0HvTeNHN
"""

!pip install stanza
import stanza
import sqlite3

con = sqlite3.connect("content/proyectoPura.db")
cur = con.cursor()

#We create a query to extract all the titles from one column
#from different tables OBRAS, COMPLEMENTOS Y SECCIONES:
query = """
SELECT Título_Obra FROM OBRAS
UNION ALL
SELECT Título_Complemento FROM COMPLEMENTOS
UNION ALL
SELECT Título_Sección FROM SECCIONES_PUBLICADAS
"""
#we reunite them:
cur.execute(query)
titles = cur.fetchall()
#print(titles)

#they return us titles in a list of tuples,
#but we need a list of strings to create easier a .txt with all of them as rows
titles_list = []
for title in titles:
  if title[0] is not None:  #some rows are NULL, we avoid them
    titles_list.append(title[0])
print(titles_list)

with open("content/ALL_TITLES.txt", "w", encoding="utf-8") as f:
  for elemento in titles_list:
    f.write(elemento + "\n")

con.close()

def processing_1(text, nlp_model):
  doc = nlp_model(text)
  return [(word.text, word.lemma, word.upos, word.head, word.deprel) for sent in doc.sentences for word in sent.words]

nlp_es = stanza.Pipeline(lang="es", processors="tokenize,mwt,pos,lemma,ner,depparse")
path = "content/ALL_TITLES.txt"

with open(path, "r", encoding="utf-8") as f:
  rows = f.readlines()

processed_titles_es = [processing_1(row, nlp_es) for row in rows]

processed_titles_es

with open("content/_ES_PROCESSING_PERTITLE.txt", "w", encoding="utf-8") as f:
  for element in processed_titles_es:
    f.write(str(element) + "\n")

# Before applying NER, first cleaning of each title
# (only specific symbols that affect NER, such us "[...]", "-", ".") %%

import re
def clean_text(text):
  cleaned_text = re.sub(r'\s*"\s*|\s*-\s*|\s*\[\.\.\.\]\s*', ' ', text)
  cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
  return cleaned_text

path = "content/ALL_TITLES.txt"
with open(path, "r", encoding="utf-8") as f:
  rows = f.readlines()
  cleaned_rows = [clean_text(row) for row in rows]

print(cleaned_rows)

with open("content/CLEAN_ALL_TITLES.txt", "w", encoding="utf-8") as f:
    for element in cleaned_rows:
        f.write(str(element) + "\n")

#let's iterate on cleaned_rows through NER function

def ner1(text, nlp_model):
  doc = nlp_model(text)
  return [(ent.text, ent.type) for ent in doc.entities]

nlp_es = stanza.Pipeline(lang="es", processors="tokenize,mwt,pos,lemma,ner")

cleaned_entities_ES = [ner1(title, nlp_es) for title in cleaned_rows]

print(cleaned_entities_ES)

with open("content/CLEANED_ENTITIES_ES.txt", "w", encoding="utf-8") as f:
    for element in cleaned_entities_ES:
        f.write(str(element) + "\n")

# and now, we also want to process (tok, lemma, mkw, pos) the NER string,
# without applying depparse %%
def processing_2(text, nlp_model):
  doc = nlp_model(text)
  return [(word.text, word.lemma, word.upos) for sent in doc.sentences for word in sent.words]

clean_processed_entities_ES = []

for entity_list in cleaned_entities_ES:
  row_result = []
  for entity in entity_list:
    text = entity[0]
    tok = processing_2(text, nlp_es)
    row_result.append([entity, tok])

  clean_processed_entities_ES.append(row_result) #IMPORTANT TO NOT INDENT THIS LINE, OTHERWISE IT DOES NOT KEEP EMPTY ROWS(THAT IS, WITHOUT ENTITY)

with open("content/CLEAN_MERGED_TITLE_NER.txt", "w", encoding="utf-8") as f:
    for element in clean_processed_entities_ES:
        f.write(str(element) + "\n")

clean_processed_entities_ES[:10]

# let's locate the entire entity and its processing [[(),[()]]] where their
# strings are located within each title %%
import re
#clean_processed_entities_ES
cleaned_rows_copy = cleaned_rows.copy()

for i, titulo in enumerate(clean_processed_entities_ES):
  for componentes in titulo:
    for tuple_item in componentes:
      entity_str = tuple_item[0][0]
      if entity_str in cleaned_rows_copy[i]:
        replacement_str = f"{tuple_item}"
        cleaned_rows_copy[i] = re.sub(rf'\b{entity_str}\b', replacement_str, cleaned_rows_copy[i])

print("\n".join(cleaned_rows_copy))